{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97K2hqENP8ih"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet allennlp\n",
    "!git clone https://github.com/mhagiwara/realworldnlp.git\n",
    "%cd realworldnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "DwQUG07eWo35",
    "outputId": "8bc29048-7861-4f97-dad5-46ee6e15e50f"
   },
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization. \n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cs7S182fP8ir"
   },
   "outputs": [],
   "source": [
    "# '''  Part 0 : Build Sentiment Classifier   '''\n",
    "\n",
    "# Import necessary modules to build model \n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from allennlp.data.dataset_readers.stanford_sentiment_tree_bank import \\\n",
    "    StanfordSentimentTreeBankDatasetReader\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "from realworldnlp.predictors import SentenceClassifierPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKpPwuXMP8iu"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6D0QWxmvP8ix"
   },
   "outputs": [],
   "source": [
    "# Model in AllenNLP represents a model that is trained.\n",
    "@Model.register(\"lstm_classifier\")\n",
    "class LstmClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 vocab: Vocabulary,\n",
    "                 positive_label: str = '4') -> None:\n",
    "        super().__init__(vocab)\n",
    "        # We need the embeddings to convert word IDs to their vector representations\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # After converting a sequence of vectors to a single vector, we feed it into\n",
    "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
    "        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                      out_features=vocab.get_vocab_size('labels'))\n",
    "\n",
    "        # Monitor the metrics - we use accuracy, as well as prec, rec, f1 for 4 (very positive)\n",
    "        positive_index = vocab.get_token_index(positive_label, namespace='labels')\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        self.f1_measure = F1Measure(positive_index)\n",
    "\n",
    "        # We use the cross entropy loss because this is a classification task.\n",
    "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
    "        # which makes it unnecessary to add a separate softmax layer.\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Instances are fed to forward after batching.\n",
    "    # Fields are passed through arguments with the same name.\n",
    "    def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
    "        # shorter sequences get padded with zeros to make them equal length.\n",
    "        # Masking is the process to ignore extra zeros added by padding\n",
    "        mask = get_text_field_mask(tokens)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.linear(encoder_out)\n",
    "\n",
    "        # In AllenNLP, the output of forward() is a dictionary.\n",
    "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            self.f1_measure(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        precision, recall, f1_measure = self.f1_measure.get_metric(reset)\n",
    "        return {'accuracy': self.accuracy.get_metric(reset),\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_measure': f1_measure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcUYcm2BP8i0"
   },
   "outputs": [],
   "source": [
    "reader = StanfordSentimentTreeBankDatasetReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "Fvpo7LUJP8i5",
    "outputId": "4eebc853-c456-4f0c-e520-77454db73bcf"
   },
   "outputs": [],
   "source": [
    "train_dataset = reader.read('https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/train.txt')\n",
    "dev_dataset = reader.read('https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pShiMVOhP8i_",
    "outputId": "542c44d1-ca3e-4144-d72a-9b20f1465a3e"
   },
   "outputs": [],
   "source": [
    "# You can optionally specify the minimum count of tokens/labels.\n",
    "# `min_count={'tokens':3}` here means that any tokens that appear less than three times\n",
    "# will be ignored and not included in the vocabulary.\n",
    "vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
    "                                  min_count={'tokens': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z7ttOYT-P8jI"
   },
   "outputs": [],
   "source": [
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UyELoq7P8jN"
   },
   "outputs": [],
   "source": [
    "# BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,\n",
    "# not for labels, which are used as-is as the \"answer\" of the sentence classification\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lxafg_5EP8jS"
   },
   "outputs": [],
   "source": [
    "# Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
    "# (usually a sequence of embedded word vectors), processes it, and returns a single\n",
    "# vector. Oftentimes this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
    "# AllenNLP also supports CNNs and other simple architectures (for example,\n",
    "# just averaging over the input vectors).\n",
    "encoder = PytorchSeq2VecWrapper(\n",
    "    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjcqUX6AP8jX"
   },
   "outputs": [],
   "source": [
    "model = LstmClassifier(word_embeddings, encoder, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c3YYhI2FP8jc"
   },
   "outputs": [],
   "source": [
    "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FOctB14AP8jg"
   },
   "outputs": [],
   "source": [
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKAbI5n7P8jm"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bwTAmrGqP8jq",
    "outputId": "858a97fb-5323-4db9-a73b-647a461a3afb"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=dev_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=20)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RxVo-GFnvL48"
   },
   "outputs": [],
   "source": [
    "# '''   Part 1: Compute Sentiment Score ''' \n",
    "\n",
    "# Computes sentiment score graded 0-4 on text submitted by actors.   \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "df = pd.read_excel('/content/drive/My Drive/EUCovenant2019_for_Jonas_analysis_complete_cases_111319_Edited.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "#predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/sst-2-basic-classifier-glove-2019.06.27.tar.gz\")\n",
    "# This is the model but we built it and trained it before \n",
    "\n",
    "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
    "\n",
    "\n",
    "sent_score = [] \n",
    "\n",
    "for i in df.index:\n",
    "  if str(df['all_text_for_analysis'][i]) == 'nan':\n",
    "    string =  '-'\n",
    "    logits = predictor.predict(string)['logits']\n",
    "    label_id = np.argmax(logits)\n",
    "    sent_score = np.append(sent_score, model.vocab.get_token_from_index(label_id, 'labels'))\n",
    "  else:\n",
    "    string = df['all_text_for_analysis'][i]\n",
    "    logits = predictor.predict(string)['logits']\n",
    "    label_id = np.argmax(logits)\n",
    "    sent_score = np.append(sent_score, model.vocab.get_token_from_index(label_id, 'labels'))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "qjSsAkVTK1xB",
    "outputId": "7674d69c-5ce7-44b4-8d5c-81e26185adca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# '''  Part 2:  Consistency Score  '''  \n",
    "\n",
    "# Checks if actors are consistent with their plan/policies or not. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "df = pd.read_excel('/content/drive/My Drive/EUCovenant2019_for_Jonas_analysis_complete_cases_111319_Edited.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "peta_score = []   \n",
    "consistency_score = [] \n",
    "for i in df.index:\n",
    "  promised_rate = (df['percent_reduction'][i]*df['baseline_emissions'][i] /100 )/(  df['target_year'][i]  - df['baseline_year'][i]) \n",
    "  current_rate  = ( df['baseline_emissions'][i] - df['total_co2_emissions'][i]  )/ ( df['total_co2_emissions_year'][i] - df['baseline_year'][i]   )\n",
    "  peta_score =  np.append(peta_score ,( current_rate/promised_rate))\n",
    "\n",
    "for i in range(0,len(peta_score) ):\n",
    "  if peta_score[i] <= 1: \n",
    "    consistency_score = np.append(consistency_score, 0)\n",
    "  else: \n",
    "    consistency_score = np.append(consistency_score, 1)  \n",
    "\n",
    "avg_peta =  np.average(peta_score)\n",
    "\n",
    "# Notes: \n",
    "\n",
    "# promised_rate  =    (percent_reduction*baseline_emission / 100 )/(target_year - baseline_year) \n",
    "# current_rate   =    (baseline_emission - total_co2_emissions)/(total_co2_emissions_year - baseline_year)\n",
    "\n",
    "# if current_rate >= promised_rate:\n",
    "#   consistency score = 1\n",
    "# else: \n",
    "#   consistency score =  0    \n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of sst_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
